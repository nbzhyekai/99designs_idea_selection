{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# competition_url = \"https://99designs.hk/logo-design/contests/logo-design-wanted-pure-water-technology-63431\"\n",
    "# competition_url = \"https://99designs.hk/logo-design/contests/logo-design-appointmentpost-63470\"\n",
    "# competition_url = \"https://99designs.hk/logo-design/contests/eye-pleasing-logo-weight-wellness-reset-program-transform-1193088\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"C:\\\\Users\\\\support\\\\Downloads\\\\chromedriver_win32\\\\chromedriver.exe\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape(competition_url):\n",
    "\n",
    "    def make_soup(competition_url, page_number, active=True):\n",
    "\n",
    "        kv = {'user-agent': 'Mozilla/5.0'}\n",
    "        if active:\n",
    "            url = competition_url + \"/entries?filter=active&page=\" + str(page_number)\n",
    "        else: \n",
    "            url = competition_url + \"/entries?filter=non_active&page=\" + str(page_number)\n",
    "\n",
    "        r = requests.get(url, headers=kv, timeout=30)\n",
    "        r.raise_for_status()\n",
    "        r.status_code\n",
    "        r.encoding = r.apparent_encoding\n",
    "        soup = BeautifulSoup(r.text, 'html.parser')\n",
    "        return soup\n",
    "\n",
    "\n",
    "\n",
    "    def get_time(soup):\n",
    "        ## get time\n",
    "        # text = str(soup.find(name='div', attrs='entry-pane__results').contents)\n",
    "        text = str(soup.find(name='div', attrs='entry-pane__results'))\n",
    "        time = re.findall('\"timeCreatedString\":\".{20,30}\"', text)\n",
    "        if time:\n",
    "            time = list(map(lambda x: x.replace('\"timeCreatedString\":\"', '').replace('\",\"', \"\"), time))\n",
    "            return time\n",
    "\n",
    "        time = re.findall('timeCreatedString\\&quot\\;\\:&quot;.{20,30}\\&quot\\;', text)\n",
    "        if time:\n",
    "            time = list(map(lambda x: x[30: -6], time))\n",
    "            return time\n",
    "\n",
    "    participants_user_ids = []\n",
    "    participants_entry_ids = []\n",
    "    participants_entry_image_urls = []\n",
    "    participants_submission_time = []\n",
    "\n",
    "    def get_participant_and_entry_info(soup):\n",
    "\n",
    "        entry_matrix = soup.findAll(name='div', attrs='entry-matrix__item matrix__item')\n",
    "        for entry in entry_matrix:\n",
    "            result = entry.find(name='div', attrs='entry entry--linked entry--zoom-linked')\n",
    "            try:\n",
    "                participants_user_ids.append(result[\"data-user-id\"])\n",
    "                participants_entry_ids.append(result[\"id\"])\n",
    "                participants_entry_image_urls.append(result.find(name='a')[\"href\"])\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "    def get_ratings(competition_url, page_number, active=True):\n",
    "        if active:\n",
    "            url = competition_url + \"/entries?filter=active&page=\" + str(page_number)\n",
    "        else: \n",
    "            url = competition_url + \"/entries?filter=non_active&page=\" + str(page_number)\n",
    "        \n",
    "        driver = webdriver.Chrome(PATH)\n",
    "        driver.get(url)\n",
    "        entry_information = driver.find_element(By.XPATH, '/html/body/div[1]/div[1]/div[3]/div/div/div[2]/div[5]/div[1]/div/div').get_attribute('data-design-collection')\n",
    "        ratings = re.findall(\"\\\"rating\\\":\\d|\\\"rating\\\":false\", entry_information)\n",
    "        ratings = list(map(lambda x: x.replace(\"\\\"rating\\\":\", \"\"), ratings))\n",
    "        driver.close()\n",
    "        return ratings\n",
    "\n",
    "    # non deleted entries\n",
    "    ratings = []\n",
    "    page_number = 1\n",
    "\n",
    "\n",
    "    while True:\n",
    "        # print(page_number)\n",
    "        if page_number == 1:\n",
    "            rating = get_ratings(competition_url, page_number)\n",
    "            ratings += rating\n",
    "            if len(rating) < 36:\n",
    "                break\n",
    "        else:\n",
    "\n",
    "            rating = get_ratings(competition_url, page_number)\n",
    "            ratings += rating\n",
    "            if len(rating) < 36:\n",
    "                break\n",
    "\n",
    "\n",
    "        page_number += 1\n",
    "\n",
    "\n",
    "\n",
    "    while True:\n",
    "        if page_number == 1:\n",
    "            soup = make_soup(competition_url, page_number)\n",
    "            get_participant_and_entry_info(soup)\n",
    "            time = get_time(soup)\n",
    "            participants_submission_time += time\n",
    "            if len(time) < 36:\n",
    "                break\n",
    "        else:\n",
    "            soup = make_soup(competition_url, page_number)\n",
    "            get_participant_and_entry_info(soup)\n",
    "            time = get_time(soup)\n",
    "            participants_submission_time += time\n",
    "            if len(time) < 36:\n",
    "                break\n",
    "\n",
    "        # print(participants_user_ids)\n",
    "        # print(participants_entry_ids)\n",
    "        # print(participants_entry_image_urls)\n",
    "\n",
    "        page_number += 1\n",
    "\n",
    "\n",
    "    entries = []\n",
    "    for entry_id, participant_id, time, url, rating in zip(participants_entry_ids, \n",
    "                                                    participants_user_ids, \n",
    "                                                    participants_submission_time, \n",
    "                                                    participants_entry_image_urls,\n",
    "                                                    ratings):\n",
    "        entries.append({'entry_id': entry_id, 'participant_id': participant_id, 'time': time, 'url':url, 'rating': rating})\n",
    "\n",
    "\n",
    "    page_number = 1\n",
    "\n",
    "    def get_time_and_status_for_deleted_entry(entry_id):\n",
    "        entry_id = entry_id.split('-')[-1]\n",
    "        entry_url = competition_url + '/entries/' + entry_id\n",
    "        \n",
    "        ## make soup for entry page\n",
    "        kv = {'user-agent': 'Mozilla/5.0'}\n",
    "        r = requests.get(entry_url, headers=kv, timeout=30)\n",
    "        r.raise_for_status()\n",
    "        r.status_code\n",
    "        r.encoding = r.apparent_encoding\n",
    "        soup = BeautifulSoup(r.text, 'html.parser')\n",
    "\n",
    "        ## \n",
    "        script = str(soup.find('script', id=\"standalone-design-details-app-data\"))\n",
    "        time = re.search('\"timeCreatedString\":\".{20,30}\"', script).group(0)\n",
    "        time = time.replace('\"timeCreatedString\":\"', '').replace('\",\"', \"\")\n",
    "\n",
    "        status = 'deleted'\n",
    "        if re.search('\"status\":\"withdrawn\"', script):\n",
    "            status = 'withdrawn'\n",
    "        elif re.search('\"status\":\"eliminated\"', script):\n",
    "            status = 'declined'\n",
    "        \n",
    "\n",
    "        return time, status\n",
    "        \n",
    "\n",
    "\n",
    "    # In[33]:\n",
    "\n",
    "\n",
    "    def get_participant_and_entry_info_deleted_page(soup):\n",
    "\n",
    "        user_ids = []\n",
    "        entry_ids = []\n",
    "        status = []\n",
    "        submission_time = []\n",
    "        deleted_count = 0\n",
    "        withdrawn_count = 0\n",
    "        declined_count = 0\n",
    "\n",
    "        entry_matrix = soup.findAll(name='div', attrs='entry-matrix__item matrix__item')\n",
    "        \n",
    "        for entry in entry_matrix:\n",
    "            result = entry.find(name='div', attrs='entry')\n",
    "        # print(result)\n",
    "            try:\n",
    "                user_ids.append(result[\"data-user-id\"])\n",
    "                entry_ids.append(result[\"id\"])\n",
    "                time, s = get_time_and_status_for_deleted_entry(result[\"id\"])\n",
    "                submission_time.append(time)\n",
    "                status.append(s)\n",
    "\n",
    "                if s == 'deleted':\n",
    "                    deleted_count += 1\n",
    "                elif s == 'withdrawn':\n",
    "                    withdrawn_count += 1  \n",
    "                elif s == 'declined':\n",
    "                    declined_count += 1\n",
    "                    \n",
    "\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        return user_ids, entry_ids, status, submission_time, deleted_count, withdrawn_count, declined_count\n",
    "\n",
    "\n",
    "    # In[34]:\n",
    "\n",
    "\n",
    "    deleted_participants_user_ids = []\n",
    "    deleted_participants_entry_ids = []\n",
    "    status_all = []\n",
    "    submission_time_all = []\n",
    "    total_deleted_count = 0\n",
    "    total_withdrawn_count = 0\n",
    "    total_declined_count = 0\n",
    "\n",
    "    page_number = 1\n",
    "\n",
    "    while True:\n",
    "        # print(page_number)\n",
    "        if page_number == 1:\n",
    "            soup = make_soup(competition_url, page_number, active=False)\n",
    "            user_ids, entry_ids, status, submission_time, deleted_count, withdrawn_count, declined_count = get_participant_and_entry_info_deleted_page(soup)\n",
    "            deleted_participants_user_ids += user_ids\n",
    "            deleted_participants_entry_ids += entry_ids\n",
    "            status_all += status\n",
    "            submission_time_all += submission_time\n",
    "            total_deleted_count += deleted_count\n",
    "            total_withdrawn_count += withdrawn_count\n",
    "            total_declined_count += declined_count\n",
    "            if len(user_ids) < 36:\n",
    "                break\n",
    "        else:\n",
    "            soup = make_soup(competition_url, page_number, active=False)\n",
    "            user_ids, entry_ids, status, submission_time, deleted_count, withdrawn_count, declined_count = get_participant_and_entry_info_deleted_page(soup)\n",
    "            deleted_participants_user_ids += user_ids\n",
    "            deleted_participants_entry_ids += entry_ids\n",
    "            status_all += status\n",
    "            submission_time_all += submission_time\n",
    "            total_deleted_count += deleted_count\n",
    "            total_withdrawn_count += withdrawn_count\n",
    "            total_declined_count += declined_count\n",
    "            if len(user_ids) < 36:\n",
    "                break\n",
    "\n",
    "        page_number += 1\n",
    "\n",
    "\n",
    "    # deleted entries\n",
    "    page_number = 1\n",
    "    deleted_ratings = []\n",
    "    while True:\n",
    "        # print(page_number)\n",
    "        if page_number == 1:\n",
    "            rating = get_ratings(competition_url, page_number, False)\n",
    "            deleted_ratings += rating\n",
    "            if len(rating) < 36:\n",
    "                break\n",
    "        else:\n",
    "\n",
    "            rating = get_ratings(competition_url, page_number, False)\n",
    "            deleted_ratings += rating\n",
    "            if len(rating) < 36:\n",
    "                break\n",
    "\n",
    "\n",
    "        page_number += 1\n",
    "\n",
    "\n",
    "    deleted_entries = []\n",
    "    for entry_id, participant_id, status, submission_time, rating in zip(deleted_participants_entry_ids, \n",
    "                                        deleted_participants_user_ids, status_all, submission_time_all, deleted_ratings):\n",
    "        deleted_entries.append({'entry_id': entry_id, 'participant_id': participant_id, 'status': status, 'time': submission_time, 'rating': rating})\n",
    "\n",
    "    df_competition_description = pd.DataFrame({'url': competition_url, 'rating_entries': [entries],\n",
    "                                                    'rating_deleted_entries': [deleted_entries]})\n",
    "    \n",
    "    return df_competition_description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 59/3000 [4:03:06<284:53:28, 348.73s/it]"
     ]
    }
   ],
   "source": [
    "urls = pd.read_csv('url_2022-02-01-to-2022-10-03.csv')\n",
    "urls = urls['href'].values[:3000]\n",
    "\n",
    "results = pd.DataFrame({})\n",
    "\n",
    "for url in tqdm(urls):\n",
    "    try:\n",
    "        df = scrape(url)\n",
    "        results = pd.concat([results, df])\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_csv('contest_data_ratings_part1.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0c12256f58bf8f32a60226d33266d3233e52104bc16a76b99cdcc6182972000e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
